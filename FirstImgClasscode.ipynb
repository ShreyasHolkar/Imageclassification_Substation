{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81388a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import ViTFeatureExtractor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c6b4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44bd4343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torchvision) (2.0.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shreyas holkar\\deep cnn image classifier\\edi\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8789051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available? False\n",
      "CUDA device: None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"CUDA device:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fcf5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"E:/Dataset/timeday_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b4bee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in base_directory_images: 340\n",
      "Total number of files in base_directory_images: 340\n"
     ]
    }
   ],
   "source": [
    "directory_images_nightd = os.path.join(base_directory, \"agv_night_dark\")\n",
    "directory_annotations_nightd = os.path.join(base_directory, \"agv_night_dark_labels\")\n",
    "\n",
    "image_files = os.listdir(directory_images_nightd)\n",
    "# Print the total count of image files\n",
    "print(f\"Total number of files in base_directory_images: {len(image_files)}\")\n",
    "# Optionally, print the files themselves (can be omitted if the list is too long)\n",
    "#print(\"Files in base_directory_images:\", image_files)\n",
    "\n",
    "txt_files = os.listdir(directory_annotations_nightd)\n",
    "print(f\"Total number of files in base_directory_images: {len(txt_files)}\")\n",
    "#print(\"Files in base_directory_images:\", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8575493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in base_directory_images: 899\n",
      "Total number of files in base_directory_images: 899\n"
     ]
    }
   ],
   "source": [
    "directory_images_nightl = os.path.join(base_directory, \"agv_night_light\")\n",
    "directory_annotations_nightl = os.path.join(base_directory, \"agv_night_light_labels\")\n",
    "\n",
    "image_files = os.listdir(directory_images_nightl)\n",
    "# Print the total count of image files\n",
    "print(f\"Total number of files in base_directory_images: {len(image_files)}\")\n",
    "# Optionally, print the files themselves (can be omitted if the list is too long)\n",
    "#print(\"Files in base_directory_images:\", image_files)\n",
    "\n",
    "txt_files = os.listdir(directory_annotations_nightl)\n",
    "print(f\"Total number of files in base_directory_images: {len(txt_files)}\")\n",
    "#print(\"Files in base_directory_images:\", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e83bc8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in base_directory_images: 4030\n",
      "Total number of files in base_directory_images: 4030\n"
     ]
    }
   ],
   "source": [
    "directory_images_misc = os.path.join(base_directory, \"misc\")\n",
    "directory_annotations_misc_labels = os.path.join(base_directory, \"misc_labels\")\n",
    "\n",
    "image_files = os.listdir(directory_images_misc)\n",
    "# Print the total count of image files\n",
    "print(f\"Total number of files in base_directory_images: {len(image_files)}\")\n",
    "# Optionally, print the files themselves (can be omitted if the list is too long)\n",
    "#print(\"Files in base_directory_images:\", image_files)\n",
    "\n",
    "txt_files = os.listdir(directory_annotations_misc_labels)\n",
    "print(f\"Total number of files in base_directory_images: {len(txt_files)}\")\n",
    "#print(\"Files in base_directory_images:\", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1397f3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Debugging: Check if filenames are matching\\nimage_basenames = [os.path.splitext(f)[0] for f in image_files]  # Remove extension\\njson_basenames = [os.path.splitext(f)[0] for f in txt_files]    # Remove extension\\n\\n# Find missing JSON files\\nmissing_json_files = [f for f in image_basenames if f not in json_basenames]\\n\\nprint(f\"Total image basenames: {len(image_basenames)}\")\\nprint(f\"Total text basenames: {len(json_basenames)}\")\\nprint(\"Missing text annotations:\", missing_json_files)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Debugging: Check if filenames are matching\n",
    "image_basenames = [os.path.splitext(f)[0] for f in image_files]  # Remove extension\n",
    "json_basenames = [os.path.splitext(f)[0] for f in txt_files]    # Remove extension\n",
    "\n",
    "# Find missing JSON files\n",
    "missing_json_files = [f for f in image_basenames if f not in json_basenames]\n",
    "\n",
    "print(f\"Total image basenames: {len(image_basenames)}\")\n",
    "print(f\"Total text basenames: {len(json_basenames)}\")\n",
    "print(\"Missing text annotations:\", missing_json_files)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ac85ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None, num_classes=15):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.image_filenames = os.listdir(image_dir)\n",
    "        self.transform = transform\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_filenames[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_filename)\n",
    "        annotation_path = os.path.join(self.annotation_dir, os.path.splitext(image_filename)[0] + \".txt\")\n",
    "\n",
    "        # Load the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "        # Read the annotations and convert them to a multi-label vector\n",
    "        with open(annotation_path, 'r') as f:\n",
    "            annotations = f.readlines()\n",
    "\n",
    "        class_ids = [int(line.split()[0]) for line in annotations]\n",
    "        labels = self.create_multilabel_target(class_ids, self.num_classes)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n",
    "\n",
    "    @staticmethod\n",
    "    def create_multilabel_target(class_ids, num_classes):\n",
    "        \"\"\"\n",
    "        Converts a list of class IDs into a multi-label binary vector.\n",
    "        \"\"\"\n",
    "        label_vector = [0] * num_classes\n",
    "        for class_id in class_ids:\n",
    "            label_vector[class_id] = 1\n",
    "        return torch.tensor(label_vector, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c98f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Directories\n",
    "image_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day\"\n",
    "annotation_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day_labels\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # VGG16 expects 224x224 images\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(image_dir, annotation_dir, transform=transform, num_classes=15)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0eb3c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "655171b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Shreyas Holkar\\AppData\\Local\\Temp\\ipykernel_4884\\1773892157.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  '''class ImageClassificationDataset(Dataset):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'class ImageClassificationDataset(Dataset):\\n    def __init__(self, image_dir, annotation_dir, transform=None):\\n        self.image_dir = image_dir\\n        self.annotation_dir = annotation_dir\\n        self.transform = transform\\n        self.data = []\\n        \\n        for annotation_file in os.listdir(annotation_dir):\\n            if annotation_file.endswith(\".txt\"):\\n                image_name = annotation_file.replace(\".txt\", \".jpg\")\\n                image_path = os.path.join(image_dir, image_name)\\n                file_path = os.path.join(annotation_dir, annotation_file)\\n                \\n                if not os.path.exists(image_path):\\n                    print(f\"Skipping missing file: {image_name}\")\\n                    continue\\n                \\n                with open(file_path, \\'r\\') as f:\\n                    lines = f.readlines()\\n                    if not lines:\\n                        continue\\n                    \\n                    class_ids = [int(line.split()[0]) for line in lines]\\n                    label = max(set(class_ids), key=class_ids.count)\\n                    self.data.append((image_name, label))\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        image_name, label = self.data[idx]\\n        image_path = os.path.join(self.image_dir, image_name)\\n        image = Image.open(image_path).convert(\"RGB\")\\n        \\n        if self.transform:\\n            image = self.transform(image)\\n        \\n        return image, label\\n\\n\\n# Paths\\nimage_dir = r\"E:\\\\Dataset\\timeday_dataset\\x07gv_day\"\\nannotation_dir = r\"E:\\\\Dataset\\timeday_dataset\\x07gv_day_labels\"\\n\\n# Transformations\\ntransform = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\\n])\\n\\n# Dataset and DataLoader\\ndataset = ImageClassificationDataset(image_dir, annotation_dir, transform=transform)\\ntrain_size = int(0.8 * len(dataset))\\nval_size = len(dataset) - train_size\\n\\ntrain_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\\n\\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        for annotation_file in os.listdir(annotation_dir):\n",
    "            if annotation_file.endswith(\".txt\"):\n",
    "                image_name = annotation_file.replace(\".txt\", \".jpg\")\n",
    "                image_path = os.path.join(image_dir, image_name)\n",
    "                file_path = os.path.join(annotation_dir, annotation_file)\n",
    "                \n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Skipping missing file: {image_name}\")\n",
    "                    continue\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if not lines:\n",
    "                        continue\n",
    "                    \n",
    "                    class_ids = [int(line.split()[0]) for line in lines]\n",
    "                    label = max(set(class_ids), key=class_ids.count)\n",
    "                    self.data.append((image_name, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.data[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Paths\n",
    "image_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day\"\n",
    "annotation_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day_labels\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImageClassificationDataset(image_dir, annotation_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f2e635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=15, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# Load pre-trained VGG16\n",
    "model = models.vgg16(pretrained=True)\n",
    "\n",
    "# Adjust the output layer for 15 classes\n",
    "num_classes = 15\n",
    "model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_classes)\n",
    "\n",
    "# Print the model to verify\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f8da80c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4634\n",
      "Epoch [2/10], Loss: 0.3137\n",
      "Epoch [3/10], Loss: 0.2744\n",
      "Epoch [4/10], Loss: 0.2327\n",
      "Epoch [5/10], Loss: 0.2154\n",
      "Epoch [6/10], Loss: 0.1889\n",
      "Epoch [7/10], Loss: 0.1773\n",
      "Epoch [8/10], Loss: 0.1572\n",
      "Epoch [9/10], Loss: 0.1362\n",
      "Epoch [10/10], Loss: 0.1243\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for images, labels in dataloader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93cf454b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as vgg16_multilabel_model.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the entire model\n",
    "torch.save(model, \"vgg16_multilabel_model.pth\")\n",
    "\n",
    "print(\"Model saved as vgg16_multilabel_model.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c1a4b478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d81bdd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shreyas Holkar\\AppData\\Local\\Temp\\ipykernel_4884\\3801638789.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model = torch.load(\"vgg16_multilabel_model.pth\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Load the saved model\n",
    "model = torch.load(\"vgg16_multilabel_model.pth\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0fb86abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: ['Closed tandem disconnect switch', 'Breaker', 'Glass disc insulator', 'Porcelain pin insulator', 'Muffle', 'Recloser', 'Current transformer']\n",
      "Predicted classes: [3, 4, 6, 7, 8, 10, 12]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    '''image = Image.open(\"E:/Dataset/timeday_dataset/Combined-Current-and-Voltage-Transformers.png\").convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension'''\n",
    "\n",
    "    image = Image.open(\"E:/Dataset/timeday_dataset/distribution-substation-equipment-1.jpg\").convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    outputs = model(image)\n",
    "    probabilities = torch.sigmoid(outputs).squeeze(0)  # Apply sigmoid for multi-label probabilities\n",
    "\n",
    "    # Threshold to decide which classes are predicted (e.g., > 0.5)\n",
    "    class_names = [\n",
    "    \"COpen blade disconnect switch\", \"Closed blade disconnect switch\", \"Open tandem disconnect switch\", \"Closed tandem disconnect switch\", \"Breaker\", \n",
    "    \"Fuse disconnect switch\", \"Glass disc insulator\", \"Porcelain pin insulator\", \"Muffle\", \"Lightning arrester\", \n",
    "    \"Recloser\", \"Power transformer\", \"Current transformer\", \"Potential transformer\", \"Tripolar disconnect switch\"\n",
    "    ]\n",
    "\n",
    "    # Assuming 'predicted' is the output from the model (logits or probabilities)\n",
    "    predicted = torch.sigmoid(outputs) > 0.5  # Convert to binary predictions for multi-label\n",
    "\n",
    "    # Get the indices of classes predicted as 1 (positive prediction)\n",
    "    predicted_indices = torch.where(predicted[0] == 1)[0]  # For a single image\n",
    "\n",
    "    # Map indices to class names\n",
    "    predicted_classes = [class_names[idx] for idx in predicted_indices.tolist()]\n",
    "\n",
    "    # Print the predicted class names\n",
    "    print(\"Predicted classes:\", predicted_classes)\n",
    "\n",
    "    predicted_index = [i for i, prob in enumerate(probabilities) if prob > 0.5]\n",
    "    print(\"Predicted classes:\", predicted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b475aa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tkinter as tk\\nfrom tkinter import filedialog, messagebox\\nfrom PIL import Image, ImageTk\\nimport os\\nimport torch\\nimport torchvision.transforms as transforms\\nfrom IPython.display import display, HTML\\n\\n# GUI within Jupyter Notebook\\nclass ImageClassifierNotebookGUI:\\n    def __init__(self, root):\\n        self.root = root\\n        self.root.title(\"Image Classification GUI\")\\n\\n        # GUI Elements\\n        self.label = tk.Label(root, text=\"Select an image for classification\")\\n        self.label.pack(pady=10)\\n\\n        self.image_label = tk.Label(root)\\n        self.image_label.pack(pady=10)\\n\\n        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\\n        self.select_button.pack(pady=5)\\n\\n        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\\n        self.classify_button.pack(pady=5)\\n\\n        self.image_path = None\\n\\n    def select_image(self):\\n        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\\n        if file_path:\\n            self.image_path = file_path\\n            image = Image.open(file_path)\\n            image.thumbnail((300, 300))\\n            img = ImageTk.PhotoImage(image)\\n            self.image_label.config(image=img)\\n            self.image_label.image = img\\n            self.classify_button.config(state=tk.NORMAL)\\n\\n    def classify_image(self):\\n        if self.image_path:\\n            try:\\n                # Image preprocessing (update as per your model requirements)\\n                transform = transforms.Compose([\\n                    transforms.Resize((224, 224)),\\n                    transforms.ToTensor(),\\n                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n                ])\\n\\n                image = Image.open(self.image_path).convert(\\'RGB\\')\\n                input_tensor = transform(image).unsqueeze(0)\\n\\n                # Replace \\'model\\' with your actual PyTorch model\\n                model.eval()  # Ensure the model is in evaluation mode\\n                with torch.no_grad():\\n                    outputs = model(input_tensor)\\n                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\\n\\n                # Filter classes with probabilities >= 60%\\n                threshold = 0.6\\n                results = []\\n                for i, prob in enumerate(probabilities):\\n                    if prob.item() >= threshold:\\n                        class_name = class_names[i]\\n                        results.append(f\"{class_name}: {prob.item() * 100:.2f}%\")\\n\\n                # Display the result\\n                if results:\\n                    result_message = \"\\n\".join(results)\\n                    messagebox.showinfo(\"Classification Results\", f\"Predicted Classes (>= 60%):\\n{result_message}\")\\n                else:\\n                    messagebox.showinfo(\"Classification Results\", \"No classes identified with >= 60% confidence.\")\\n\\n            except Exception as e:\\n                messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\\n\\n# Run the GUI\\nif __name__ == \"__main__\":\\n    root = tk.Tk()\\n    app = ImageClassifierNotebookGUI(root)\\n    root.mainloop()'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# GUI within Jupyter Notebook\n",
    "class ImageClassifierNotebookGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Classification GUI\")\n",
    "\n",
    "        # GUI Elements\n",
    "        self.label = tk.Label(root, text=\"Select an image for classification\")\n",
    "        self.label.pack(pady=10)\n",
    "\n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack(pady=10)\n",
    "\n",
    "        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\n",
    "        self.select_button.pack(pady=5)\n",
    "\n",
    "        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\n",
    "        self.classify_button.pack(pady=5)\n",
    "\n",
    "        self.image_path = None\n",
    "\n",
    "    def select_image(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "        if file_path:\n",
    "            self.image_path = file_path\n",
    "            image = Image.open(file_path)\n",
    "            image.thumbnail((300, 300))\n",
    "            img = ImageTk.PhotoImage(image)\n",
    "            self.image_label.config(image=img)\n",
    "            self.image_label.image = img\n",
    "            self.classify_button.config(state=tk.NORMAL)\n",
    "\n",
    "    def classify_image(self):\n",
    "        if self.image_path:\n",
    "            try:\n",
    "                # Image preprocessing (update as per your model requirements)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "                image = Image.open(self.image_path).convert('RGB')\n",
    "                input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "                # Replace 'model' with your actual PyTorch model\n",
    "                model.eval()  # Ensure the model is in evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "                # Filter classes with probabilities >= 60%\n",
    "                threshold = 0.6\n",
    "                results = []\n",
    "                for i, prob in enumerate(probabilities):\n",
    "                    if prob.item() >= threshold:\n",
    "                        class_name = class_names[i]\n",
    "                        results.append(f\"{class_name}: {prob.item() * 100:.2f}%\")\n",
    "\n",
    "                # Display the result\n",
    "                if results:\n",
    "                    result_message = \"\\n\".join(results)\n",
    "                    messagebox.showinfo(\"Classification Results\", f\"Predicted Classes (>= 60%):\\n{result_message}\")\n",
    "                else:\n",
    "                    messagebox.showinfo(\"Classification Results\", \"No classes identified with >= 60% confidence.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the GUI\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ImageClassifierNotebookGUI(root)\n",
    "    root.mainloop()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69ed612a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# GUI within Jupyter Notebook\n",
    "class ImageClassifierNotebookGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Classification GUI\")\n",
    "\n",
    "        # GUI Elements\n",
    "        self.label = tk.Label(root, text=\"Select an image for classification\")\n",
    "        self.label.pack(pady=10)\n",
    "\n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack(pady=10)\n",
    "\n",
    "        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\n",
    "        self.select_button.pack(pady=5)\n",
    "\n",
    "        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\n",
    "        self.classify_button.pack(pady=5)\n",
    "\n",
    "        self.image_path = None\n",
    "\n",
    "    def select_image(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "        if file_path:\n",
    "            self.image_path = file_path\n",
    "            image = Image.open(file_path)\n",
    "            image.thumbnail((300, 300))\n",
    "            img = ImageTk.PhotoImage(image)\n",
    "            self.image_label.config(image=img)\n",
    "            self.image_label.image = img\n",
    "            self.classify_button.config(state=tk.NORMAL)\n",
    "\n",
    "    def classify_image(self):\n",
    "        if self.image_path:\n",
    "            try:\n",
    "                # Image preprocessing (update as per your model requirements)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "                image = Image.open(self.image_path).convert('RGB')\n",
    "                input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "                # Replace 'model' with your actual PyTorch model\n",
    "                model.eval()  # Ensure the model is in evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "                # Get top classes and their probabilities\n",
    "                top_probabilities, top_indices = torch.topk(probabilities, len(class_names))\n",
    "\n",
    "                results = []\n",
    "                for i in range(len(top_indices)):\n",
    "                    class_id = top_indices[i].item()\n",
    "                    class_name = class_names[class_id]\n",
    "                    prob = top_probabilities[i].item()\n",
    "                    results.append(f\"{class_name}: {prob:.4f}\")\n",
    "\n",
    "                # Display the result\n",
    "                result_message = \"\\n\".join(results)\n",
    "                messagebox.showinfo(\"Classification Results\", f\"Predicted Classes:\\n{result_message}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the GUI\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ImageClassifierNotebookGUI(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717034c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
