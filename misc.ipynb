{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
    "from transformers import ViTFeatureExtractor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import torch\n",
    "from PIL import Image\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = \"E:/Dataset/timeday_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in base_directory_images: 4030\n",
      "Total number of files in base_directory_images: 4030\n"
     ]
    }
   ],
   "source": [
    "directory_images_misc = os.path.join(base_directory, \"misc\")\n",
    "directory_annotations_misc_labels = os.path.join(base_directory, \"misc_labels\")\n",
    "\n",
    "image_files = os.listdir(directory_images_misc)\n",
    "# Print the total count of image files\n",
    "print(f\"Total number of files in base_directory_images: {len(image_files)}\")\n",
    "# Optionally, print the files themselves (can be omitted if the list is too long)\n",
    "#print(\"Files in base_directory_images:\", image_files)\n",
    "\n",
    "txt_files = os.listdir(directory_annotations_misc_labels)\n",
    "print(f\"Total number of files in base_directory_images: {len(txt_files)}\")\n",
    "#print(\"Files in base_directory_images:\", txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Debugging: Check if filenames are matching\\nimage_basenames = [os.path.splitext(f)[0] for f in image_files]  # Remove extension\\njson_basenames = [os.path.splitext(f)[0] for f in txt_files]    # Remove extension\\n\\n# Find missing JSON files\\nmissing_json_files = [f for f in image_basenames if f not in json_basenames]\\n\\nprint(f\"Total image basenames: {len(image_basenames)}\")\\nprint(f\"Total text basenames: {len(json_basenames)}\")\\nprint(\"Missing text annotations:\", missing_json_files)'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Debugging: Check if filenames are matching\n",
    "image_basenames = [os.path.splitext(f)[0] for f in image_files]  # Remove extension\n",
    "json_basenames = [os.path.splitext(f)[0] for f in txt_files]    # Remove extension\n",
    "\n",
    "# Find missing JSON files\n",
    "missing_json_files = [f for f in image_basenames if f not in json_basenames]\n",
    "\n",
    "print(f\"Total image basenames: {len(image_basenames)}\")\n",
    "print(f\"Total text basenames: {len(json_basenames)}\")\n",
    "print(\"Missing text annotations:\", missing_json_files)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: 20210713_144524.txt is empty.\n",
      "Warning: 20210713_144557.txt is empty.\n",
      "Warning: 20210713_144753.txt is empty.\n",
      "Warning: 20210713_144811.txt is empty.\n",
      "Warning: 20210713_144845.txt is empty.\n",
      "Warning: 20210713_150012.txt is empty.\n",
      "Warning: FLIR0505.txt is empty.\n",
      "Warning: FLIR1875_rgb.txt is empty.\n",
      "Warning: IMG_20201215_140429_8.txt is empty.\n",
      "Warning: PANO0001 (9).txt is empty.\n",
      "Warning: PANO0007 (9).txt is empty.\n",
      "Warning: PANO0010.txt is empty.\n",
      "Warning: PANO0012 (2).txt is empty.\n",
      "Warning: PANO0015 (2).txt is empty.\n",
      "Warning: PANO0018 (2).txt is empty.\n",
      "Warning: PANO0033.txt is empty.\n",
      "Dataset CSV saved to misc_labels.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "images_path = \"E:/Dataset/timeday_dataset/misc\"\n",
    "annotations_path = \"E:/Dataset/timeday_dataset/misc_labels\"\n",
    "output_csv = \"misc_labels.csv\"\n",
    "\n",
    "# Initialize list to store data\n",
    "data = []\n",
    "\n",
    "# Process each annotation file\n",
    "for annotation_file in os.listdir(annotations_path):\n",
    "    if annotation_file.endswith(\".txt\"):\n",
    "        image_name = annotation_file.replace(\".txt\", \".jpg\")  # Assuming image is .jpg\n",
    "        file_path = os.path.join(annotations_path, annotation_file)\n",
    "        \n",
    "        with open(file_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if not lines:\n",
    "                print(f\"Warning: {annotation_file} is empty.\")\n",
    "                continue\n",
    "\n",
    "            # Extract class IDs\n",
    "            try:\n",
    "                class_ids = [int(line.split()[0]) for line in lines]\n",
    "                # Choose label strategy: Most frequent class\n",
    "                label = max(set(class_ids), key=class_ids.count)\n",
    "                # Append to data\n",
    "                data.append([image_name, label])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {annotation_file}: {e}\")\n",
    "\n",
    "# Check if data is populated\n",
    "if not data:\n",
    "    print(\"No data found. Please check your annotation files and paths.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    df = pd.DataFrame(data, columns=[\"image\", \"label\"])\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    print(f\"Dataset CSV saved to {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping missing file: 754.jpg\n",
      "Skipping missing file: 755.jpg\n",
      "Skipping missing file: 758.jpg\n",
      "Skipping missing file: 762.jpg\n",
      "Skipping missing file: 763.jpg\n",
      "Skipping missing file: 764.jpg\n",
      "Skipping missing file: 765.jpg\n",
      "Skipping missing file: 767.jpg\n",
      "Skipping missing file: IMG_0949.jpg\n",
      "Skipping missing file: IMG_0950.jpg\n",
      "Skipping missing file: IMG_0951.jpg\n",
      "Skipping missing file: IMG_0952.jpg\n",
      "Skipping missing file: IMG_0953.jpg\n",
      "Skipping missing file: IMG_0954.jpg\n",
      "Skipping missing file: IMG_0955.jpg\n",
      "Skipping missing file: IMG_0956.jpg\n",
      "Skipping missing file: IMG_0957.jpg\n",
      "Skipping missing file: IMG_0958.jpg\n",
      "Skipping missing file: IMG_0960.jpg\n",
      "Skipping missing file: IMG_0961.jpg\n",
      "Skipping missing file: IMG_0964.jpg\n",
      "Skipping missing file: IMG_0965.jpg\n",
      "Skipping missing file: IMG_0966.jpg\n",
      "Skipping missing file: IMG_0967.jpg\n",
      "Skipping missing file: IMG_0968.jpg\n",
      "Skipping missing file: IMG_0970.jpg\n",
      "Skipping missing file: IMG_0971.jpg\n",
      "Skipping missing file: IMG_0972.jpg\n",
      "Skipping missing file: IMG_0973.jpg\n",
      "Skipping missing file: IMG_0974.jpg\n",
      "Skipping missing file: IMG_0976.jpg\n",
      "Skipping missing file: IMG_0977.jpg\n",
      "Skipping missing file: IMG_0978.jpg\n",
      "Skipping missing file: IMG_0979.jpg\n",
      "Skipping missing file: IMG_0980.jpg\n",
      "Skipping missing file: IMG_0981.jpg\n",
      "Skipping missing file: IMG_0982.jpg\n",
      "Skipping missing file: IMG_0983.jpg\n",
      "Skipping missing file: IMG_0984.jpg\n",
      "Skipping missing file: IMG_0985.jpg\n",
      "Skipping missing file: IMG_0986.jpg\n",
      "Skipping missing file: IMG_0987.jpg\n",
      "Skipping missing file: IMG_0988.jpg\n",
      "Skipping missing file: IMG_0989.jpg\n",
      "Skipping missing file: IMG_0990.jpg\n",
      "Skipping missing file: IMG_0991.jpg\n",
      "Skipping missing file: IMG_0992.jpg\n",
      "Skipping missing file: IMG_0993.jpg\n",
      "Skipping missing file: IMG_0994.jpg\n",
      "Skipping missing file: IMG_0995.jpg\n",
      "Skipping missing file: IMG_0996.jpg\n",
      "Skipping missing file: IMG_0997.jpg\n",
      "Skipping missing file: IMG_0998.jpg\n",
      "Skipping missing file: IMG_0999.jpg\n",
      "Skipping missing file: IMG_1001.jpg\n",
      "Skipping missing file: IMG_1002.jpg\n",
      "Skipping missing file: IMG_1003.jpg\n",
      "Skipping missing file: IMG_1004.jpg\n",
      "Skipping missing file: IMG_1005.jpg\n",
      "Skipping missing file: IMG_1006.jpg\n",
      "Skipping missing file: IMG_1007.jpg\n",
      "Skipping missing file: IMG_1008.jpg\n",
      "Skipping missing file: IMG_1009.jpg\n",
      "Skipping missing file: IMG_1010.jpg\n",
      "Skipping missing file: IMG_1011.jpg\n",
      "Skipping missing file: IMG_1012.jpg\n",
      "Skipping missing file: IMG_1013.jpg\n",
      "Skipping missing file: IMG_1014.jpg\n",
      "Skipping missing file: IMG_1015.jpg\n",
      "Skipping missing file: IMG_1016.jpg\n",
      "Skipping missing file: IMG_1017.jpg\n",
      "Skipping missing file: IMG_1018.jpg\n",
      "Skipping missing file: IMG_1019.jpg\n",
      "Skipping missing file: IMG_1025.jpg\n",
      "Skipping missing file: IMG_1026.jpg\n",
      "Skipping missing file: IMG_1027.jpg\n",
      "Skipping missing file: IMG_1028.jpg\n",
      "Skipping missing file: IMG_1029.jpg\n",
      "Skipping missing file: IMG_1030.jpg\n",
      "Skipping missing file: IMG_1031.jpg\n",
      "Skipping missing file: IMG_1032.jpg\n",
      "Skipping missing file: IMG_1033.jpg\n",
      "Skipping missing file: IMG_1035.jpg\n",
      "Skipping missing file: IMG_1037.jpg\n",
      "Skipping missing file: IMG_1038.jpg\n",
      "Skipping missing file: IMG_1039.jpg\n",
      "Skipping missing file: IMG_1040.jpg\n",
      "Skipping missing file: IMG_1041.jpg\n",
      "Skipping missing file: IMG_1042.jpg\n",
      "Skipping missing file: IMG_1043.jpg\n",
      "Skipping missing file: IMG_1047.jpg\n",
      "Skipping missing file: IMG_1049.jpg\n",
      "Skipping missing file: IMG_1050.jpg\n",
      "Skipping missing file: IMG_1052.jpg\n",
      "Skipping missing file: IMG_1053.jpg\n",
      "Skipping missing file: IMG_1054.jpg\n",
      "Skipping missing file: IMG_1055.jpg\n",
      "Skipping missing file: IMG_1056.jpg\n",
      "Skipping missing file: IMG_1057.jpg\n",
      "Skipping missing file: IMG_1058.jpg\n",
      "Skipping missing file: IMG_1059.jpg\n",
      "Skipping missing file: IMG_1060.jpg\n",
      "Skipping missing file: IMG_1061.jpg\n",
      "Skipping missing file: IMG_1062.jpg\n",
      "Skipping missing file: IMG_1063.jpg\n",
      "Skipping missing file: IMG_1064.jpg\n",
      "Skipping missing file: IMG_1065.jpg\n",
      "Skipping missing file: IMG_1066.jpg\n",
      "Skipping missing file: IMG_1067.jpg\n",
      "Skipping missing file: IMG_1068.jpg\n",
      "Skipping missing file: IMG_1069.jpg\n",
      "Skipping missing file: IMG_1070.jpg\n",
      "Skipping missing file: IMG_1071.jpg\n",
      "Skipping missing file: IMG_1072.jpg\n",
      "Skipping missing file: IMG_1073.jpg\n",
      "Skipping missing file: IMG_1074.jpg\n",
      "Skipping missing file: IMG_1075.jpg\n",
      "Skipping missing file: IMG_1076.jpg\n",
      "Skipping missing file: IMG_1077.jpg\n",
      "Skipping missing file: IMG_1078.jpg\n",
      "Skipping missing file: IMG_1079.jpg\n",
      "Skipping missing file: IMG_1080.jpg\n",
      "Skipping missing file: IMG_1081.jpg\n",
      "Skipping missing file: IMG_1082.jpg\n",
      "Skipping missing file: IMG_1083.jpg\n",
      "Skipping missing file: IMG_1084.jpg\n",
      "Skipping missing file: IMG_1085.jpg\n",
      "Skipping missing file: IMG_1086.jpg\n",
      "Skipping missing file: IMG_1087.jpg\n",
      "Skipping missing file: IMG_1088.jpg\n",
      "Skipping missing file: IMG_1089.jpg\n",
      "Skipping missing file: IMG_1090.jpg\n",
      "Skipping missing file: IMG_1091.jpg\n",
      "Skipping missing file: IMG_1092.jpg\n",
      "Skipping missing file: IMG_1093.jpg\n",
      "Skipping missing file: IMG_1094.jpg\n",
      "Skipping missing file: IMG_1095.jpg\n",
      "Skipping missing file: IMG_1096.jpg\n",
      "Skipping missing file: IMG_1097.jpg\n",
      "Skipping missing file: IMG_1098.jpg\n",
      "Skipping missing file: IMG_1100.jpg\n",
      "Skipping missing file: IMG_1101.jpg\n",
      "Skipping missing file: IMG_1102.jpg\n",
      "Skipping missing file: IMG_1103.jpg\n",
      "Skipping missing file: IMG_1104.jpg\n",
      "Skipping missing file: IMG_1105.jpg\n",
      "Skipping missing file: IMG_1106.jpg\n",
      "Skipping missing file: IMG_1107.jpg\n",
      "Skipping missing file: IMG_1108.jpg\n",
      "Skipping missing file: IMG_1109.jpg\n",
      "Skipping missing file: IMG_1110.jpg\n",
      "Skipping missing file: IMG_1111.jpg\n",
      "Skipping missing file: IMG_1112.jpg\n",
      "Skipping missing file: IMG_1113.jpg\n",
      "Skipping missing file: IMG_1114.jpg\n",
      "Skipping missing file: IMG_1115.jpg\n",
      "Skipping missing file: IMG_1117.jpg\n",
      "Skipping missing file: IMG_1118.jpg\n",
      "Skipping missing file: IMG_1119.jpg\n",
      "Skipping missing file: IMG_1120.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 11.54.06.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 11.58.23.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 11.59.12.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 12.01.42.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 12.02.36.jpg\n",
      "Skipping missing file: WhatsApp Image 2021-07-21 at 12.05.39.jpg\n"
     ]
    }
   ],
   "source": [
    "class ImageClassificationDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotation_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.annotation_dir = annotation_dir\n",
    "        self.transform = transform\n",
    "        self.data = []\n",
    "        \n",
    "        for annotation_file in os.listdir(annotation_dir):\n",
    "            if annotation_file.endswith(\".txt\"):\n",
    "                image_name = annotation_file.replace(\".txt\", \".jpg\")\n",
    "                image_path = os.path.join(image_dir, image_name)\n",
    "                file_path = os.path.join(annotation_dir, annotation_file)\n",
    "                \n",
    "                if not os.path.exists(image_path):\n",
    "                    print(f\"Skipping missing file: {image_name}\")\n",
    "                    continue\n",
    "                \n",
    "                with open(file_path, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                    if not lines:\n",
    "                        continue\n",
    "                    \n",
    "                    class_ids = [int(line.split()[0]) for line in lines]\n",
    "                    label = max(set(class_ids), key=class_ids.count)\n",
    "                    self.data.append((image_name, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name, label = self.data[idx]\n",
    "        image_path = os.path.join(self.image_dir, image_name)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Paths\n",
    "image_dir = \"E:/Dataset/timeday_dataset/misc\"\n",
    "annotation_dir = \"E:/Dataset/timeday_dataset/misc_labels\"\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = ImageClassificationDataset(image_dir, annotation_dir, transform=transform)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Modify the final layer for your 15 classes\n",
    "num_classes = 15\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.8226\n",
      "Epoch [2/10], Loss: 0.6350\n",
      "Epoch [3/10], Loss: 0.5961\n",
      "Epoch [4/10], Loss: 0.5602\n",
      "Epoch [5/10], Loss: 0.5601\n",
      "Epoch [6/10], Loss: 0.4949\n",
      "Epoch [7/10], Loss: 0.4243\n",
      "Epoch [8/10], Loss: 0.3776\n",
      "Epoch [9/10], Loss: 0.3376\n",
      "Epoch [10/10], Loss: 0.3158\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "C:\\Users\\Shreyas Holkar\\AppData\\Local\\Temp\\ipykernel_14216\\3197171620.py:1: SyntaxWarning: invalid escape sequence '\\D'\n",
      "  \"\"\"import os\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import os\\n\\nimage_dir = r\"E:\\\\Dataset\\timeday_dataset\\x07gv_day\"  # Replace with your path\\nannotation_dir = r\"E:\\\\Dataset\\timeday_dataset\\x07gv_day_labels\"  # Replace with your path\\n\\nmissing_files = []\\nfor annotation_file in os.listdir(annotation_dir):\\n    if annotation_file.endswith(\".txt\"):\\n        image_name = annotation_file.replace(\".txt\", \".jpg\")  # Adjust if not .jpg\\n        image_path = os.path.join(image_dir, image_name)\\n        if not os.path.exists(image_path):\\n            missing_files.append(image_name)\\n\\nif missing_files:\\n    print(f\"Missing image files: {len(missing_files)}\")\\nelse:\\n    print(\"All image files are present!\")'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import os\n",
    "\n",
    "image_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day\"  # Replace with your path\n",
    "annotation_dir = r\"E:\\Dataset\\timeday_dataset\\agv_day_labels\"  # Replace with your path\n",
    "\n",
    "missing_files = []\n",
    "for annotation_file in os.listdir(annotation_dir):\n",
    "    if annotation_file.endswith(\".txt\"):\n",
    "        image_name = annotation_file.replace(\".txt\", \".jpg\")  # Adjust if not .jpg\n",
    "        image_path = os.path.join(image_dir, image_name)\n",
    "        if not os.path.exists(image_path):\n",
    "            missing_files.append(image_name)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"Missing image files: {len(missing_files)}\")\n",
    "else:\n",
    "    print(\"All image files are present!\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"image_classification_model_v3.pth\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shreyas Holkar\\deep cnn image classifier\\edi\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Shreyas Holkar\\AppData\\Local\\Temp\\ipykernel_14216\\3591867347.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"image_classification_model.pth\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=15, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=False)\n",
    "num_classes = 15\n",
    "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(\"image_classification_model.pth\"))\n",
    "model.eval()  # Set the model to evaluation mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image belongs to class: Muffle\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0)  # Add batch dimension\n",
    "    return image\n",
    "\n",
    "# Load class labels from the text file\n",
    "def load_class_labels(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        class_labels = [line.strip() for line in f.readlines()]\n",
    "    return class_labels\n",
    "\n",
    "class_labels = load_class_labels(\"classes.txt\")\n",
    "\n",
    "\n",
    "def predict(image_path, model, class_labels):\n",
    "    image = preprocess_image(image_path)\n",
    "    outputs = model(image)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    predicted_class = class_labels[predicted.item()]  # Use loaded class labels\n",
    "    return predicted_class\n",
    "\n",
    "\n",
    "image_path = \"E:/Dataset/timeday_dataset/distribution-substation-equipment-1.jpg\"  # Replace with your image path\n",
    "\n",
    "predicted_class = predict(image_path, model, class_labels)\n",
    "print(f\"The image belongs to class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "Class 7: 3236 samples\n",
      "Class 1: 72 samples\n",
      "Class 12: 54 samples\n",
      "Class 8: 186 samples\n",
      "Class 4: 20 samples\n",
      "Class 5: 1 samples\n",
      "Class 13: 19 samples\n",
      "Class 11: 27 samples\n",
      "Class 10: 41 samples\n",
      "Class 9: 58 samples\n",
      "Class 6: 110 samples\n",
      "Class 2: 8 samples\n",
      "Class 3: 16 samples\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Analyze class distribution\n",
    "class_counts = Counter([label for _, label in dataset.data])\n",
    "print(\"Class Distribution:\")\n",
    "for cls, count in class_counts.items():\n",
    "    print(f\"Class {cls}: {count} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import tkinter as tk\\nfrom tkinter import filedialog, messagebox\\nfrom PIL import Image, ImageTk\\nimport torch\\nimport torchvision.transforms as transforms\\nfrom torchvision.models import resnet18\\n\\n# Load a pre-trained model\\nmodel = resnet18(pretrained=True)\\nmodel.eval()\\n\\n# Define the classes (for demonstration, use ImageNet classes)\\nimagenet_classes = {i: f\"Class {i}\" for i in range(1000)}\\n\\n# Image preprocessing\\ntransform = transforms.Compose([\\n    transforms.Resize((224, 224)),\\n    transforms.ToTensor(),\\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\\n])\\n\\n# GUI Application\\nclass ImageClassifierApp:\\n    def __init__(self, root):\\n        self.root = root\\n        self.root.title(\"Image Classifier\")\\n        \\n        self.label = tk.Label(root, text=\"Select an image to classify\")\\n        self.label.pack(pady=10)\\n        \\n        self.image_label = tk.Label(root)\\n        self.image_label.pack(pady=10)\\n        \\n        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\\n        self.select_button.pack(pady=5)\\n        \\n        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\\n        self.classify_button.pack(pady=5)\\n        \\n        self.image_path = None\\n    \\n    def select_image(self):\\n        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\\n        if file_path:\\n            self.image_path = file_path\\n            image = Image.open(file_path)\\n            image.thumbnail((300, 300))\\n            img = ImageTk.PhotoImage(image)\\n            self.image_label.config(image=img)\\n            self.image_label.image = img\\n            self.classify_button.config(state=tk.NORMAL)\\n    \\n    def classify_image(self):\\n        if self.image_path:\\n            image = Image.open(self.image_path)\\n            input_tensor = transform(image).unsqueeze(0)\\n            \\n            with torch.no_grad():\\n                outputs = model(input_tensor)\\n                _, predicted = torch.max(outputs, 1)\\n                class_id = predicted.item()\\n                class_name = imagenet_classes.get(class_id, \"Unknown Class\")\\n            \\n            messagebox.showinfo(\"Classification Result\", f\"Predicted Class: {class_name}\")\\n\\n# Run the application\\nif __name__ == \"__main__\":\\n    root = tk.Tk()\\n    app = ImageClassifierApp(root)\\n    root.mainloop()'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Define the classes (for demonstration, use ImageNet classes)\n",
    "imagenet_classes = {i: f\"Class {i}\" for i in range(1000)}\n",
    "\n",
    "# Image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# GUI Application\n",
    "class ImageClassifierApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Classifier\")\n",
    "        \n",
    "        self.label = tk.Label(root, text=\"Select an image to classify\")\n",
    "        self.label.pack(pady=10)\n",
    "        \n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack(pady=10)\n",
    "        \n",
    "        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\n",
    "        self.select_button.pack(pady=5)\n",
    "        \n",
    "        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\n",
    "        self.classify_button.pack(pady=5)\n",
    "        \n",
    "        self.image_path = None\n",
    "    \n",
    "    def select_image(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "        if file_path:\n",
    "            self.image_path = file_path\n",
    "            image = Image.open(file_path)\n",
    "            image.thumbnail((300, 300))\n",
    "            img = ImageTk.PhotoImage(image)\n",
    "            self.image_label.config(image=img)\n",
    "            self.image_label.image = img\n",
    "            self.classify_button.config(state=tk.NORMAL)\n",
    "    \n",
    "    def classify_image(self):\n",
    "        if self.image_path:\n",
    "            image = Image.open(self.image_path)\n",
    "            input_tensor = transform(image).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_tensor)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                class_id = predicted.item()\n",
    "                class_name = imagenet_classes.get(class_id, \"Unknown Class\")\n",
    "            \n",
    "            messagebox.showinfo(\"Classification Result\", f\"Predicted Class: {class_name}\")\n",
    "\n",
    "# Run the application\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ImageClassifierApp(root)\n",
    "    root.mainloop()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: ['Breaker', 'Porcelain pin insulator', 'Muffle']\n",
      "Predicted classes: [4, 7, 8]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    '''image = Image.open(\"E:/Dataset/timeday_dataset/Combined-Current-and-Voltage-Transformers.png\").convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension'''\n",
    "\n",
    "    image = Image.open(\"E:/Dataset/timeday_dataset/distribution-substation-equipment-1.jpg\").convert(\"RGB\")\n",
    "    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension\n",
    "\n",
    "    outputs = model(image)\n",
    "    probabilities = torch.sigmoid(outputs).squeeze(0)  # Apply sigmoid for multi-label probabilities\n",
    "\n",
    "    # Threshold to decide which classes are predicted (e.g., > 0.5)\n",
    "    class_names = [\n",
    "    \"COpen blade disconnect switch\", \"Closed blade disconnect switch\", \"Open tandem disconnect switch\", \"Closed tandem disconnect switch\", \"Breaker\", \n",
    "    \"Fuse disconnect switch\", \"Glass disc insulator\", \"Porcelain pin insulator\", \"Muffle\", \"Lightning arrester\", \n",
    "    \"Recloser\", \"Power transformer\", \"Current transformer\", \"Potential transformer\", \"Tripolar disconnect switch\"\n",
    "    ]\n",
    "\n",
    "    # Assuming 'predicted' is the output from the model (logits or probabilities)\n",
    "    predicted = torch.sigmoid(outputs) > 0.5  # Convert to binary predictions for multi-label\n",
    "\n",
    "    # Get the indices of classes predicted as 1 (positive prediction)\n",
    "    predicted_indices = torch.where(predicted[0] == 1)[0]  # For a single image\n",
    "\n",
    "    # Map indices to class names\n",
    "    predicted_classes = [class_names[idx] for idx in predicted_indices.tolist()]\n",
    "\n",
    "    # Print the predicted class names\n",
    "    print(\"Predicted classes:\", predicted_classes)\n",
    "\n",
    "    predicted_index = [i for i, prob in enumerate(probabilities) if prob > 0.5]\n",
    "    print(\"Predicted classes:\", predicted_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# GUI within Jupyter Notebook\n",
    "class ImageClassifierNotebookGUI:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Image Classification GUI\")\n",
    "\n",
    "        # GUI Elements\n",
    "        self.label = tk.Label(root, text=\"Select an image for classification\")\n",
    "        self.label.pack(pady=10)\n",
    "\n",
    "        self.image_label = tk.Label(root)\n",
    "        self.image_label.pack(pady=10)\n",
    "\n",
    "        self.select_button = tk.Button(root, text=\"Select Image\", command=self.select_image)\n",
    "        self.select_button.pack(pady=5)\n",
    "\n",
    "        self.classify_button = tk.Button(root, text=\"Classify Image\", command=self.classify_image, state=tk.DISABLED)\n",
    "        self.classify_button.pack(pady=5)\n",
    "\n",
    "        self.image_path = None\n",
    "\n",
    "    def select_image(self):\n",
    "        file_path = filedialog.askopenfilename(filetypes=[(\"Image Files\", \"*.jpg;*.jpeg;*.png\")])\n",
    "        if file_path:\n",
    "            self.image_path = file_path\n",
    "            image = Image.open(file_path)\n",
    "            image.thumbnail((300, 300))\n",
    "            img = ImageTk.PhotoImage(image)\n",
    "            self.image_label.config(image=img)\n",
    "            self.image_label.image = img\n",
    "            self.classify_button.config(state=tk.NORMAL)\n",
    "\n",
    "    def classify_image(self):\n",
    "        if self.image_path:\n",
    "            try:\n",
    "                # Image preprocessing (update as per your model requirements)\n",
    "                transform = transforms.Compose([\n",
    "                    transforms.Resize((224, 224)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                ])\n",
    "\n",
    "                image = Image.open(self.image_path).convert('RGB')\n",
    "                input_tensor = transform(image).unsqueeze(0)\n",
    "\n",
    "                # Replace 'model' with your actual PyTorch model\n",
    "                model.eval()  # Ensure the model is in evaluation mode\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(input_tensor)\n",
    "                    probabilities = torch.nn.functional.softmax(outputs[0], dim=0)\n",
    "\n",
    "                # Get top classes and their probabilities\n",
    "                top_probabilities, top_indices = torch.topk(probabilities, len(class_names))\n",
    "\n",
    "                results = []\n",
    "                for i in range(len(top_indices)):\n",
    "                    class_id = top_indices[i].item()\n",
    "                    class_name = class_names[class_id]\n",
    "                    prob = top_probabilities[i].item()\n",
    "                    results.append(f\"{class_name}: {prob:.4f}\")\n",
    "\n",
    "                # Display the result\n",
    "                result_message = \"\\n\".join(results)\n",
    "                messagebox.showinfo(\"Classification Results\", f\"Predicted Classes:\\n{result_message}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                messagebox.showerror(\"Error\", f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the GUI\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = ImageClassifierNotebookGUI(root)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
